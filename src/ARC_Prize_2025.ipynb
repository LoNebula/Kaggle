{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7761d6b",
   "metadata": {},
   "source": [
    "# ARC Prize 2025\n",
    "URL: https://www.kaggle.com/competitions/arc-prize-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccc20070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caa9c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "import itertools\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37699d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d7b4fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_solutions = json.loads(\"/kaggle/input/arc-prize-2025/arc-agi_training_solutions.json\")\n",
    "# train_challenges = json.loads(\"/kaggle/input/arc-prize-2025/arc-agi_training_challenges.json\")\n",
    "# eval_solutions = json.loads(\"/kaggle/input/arc-prize-2025/arc-agi_evaluation_solutions.json\")\n",
    "# eval_challenges = json.loads(\"/kaggle/input/arc-prize-2025/arc-agi_evaluation_challenges.json\")\n",
    "\n",
    "# file_path\n",
    "train_solutions = json.loads(open(\"data/ARC_Prize_2025/arc-agi_training_solutions.json\").read())\n",
    "train_challenges = json.loads(open(\"data/ARC_Prize_2025/arc-agi_training_challenges.json\").read())\n",
    "eval_solutions = json.loads(open(\"data/ARC_Prize_2025/arc-agi_evaluation_solutions.json\").read())\n",
    "eval_challenges = json.loads(open(\"data/ARC_Prize_2025/arc-agi_evaluation_challenges.json\").read())\n",
    "test_data = json.loads(open(\"data/ARC_Prize_2025/arc-agi_test_challenges.json\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "434ed210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max height and width: 30 30\n",
      "Max height and width: 30 30\n"
     ]
    }
   ],
   "source": [
    "# check the max height and width in challenges data\n",
    "def get_max_shape_challenge(data_dict):\n",
    "    hs, ws = [], []\n",
    "    for section in data_dict.values():\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase not in section:\n",
    "                continue\n",
    "            for pair in section[phase]:\n",
    "                if 'input' in pair:\n",
    "                    inp = torch.tensor(pair['input'])\n",
    "                    h, w = inp.shape\n",
    "                    hs.append(h)\n",
    "                    ws.append(w)\n",
    "                if 'output' in pair:\n",
    "                    out = torch.tensor(pair['output'])\n",
    "                    h, w = out.shape\n",
    "                    hs.append(h)\n",
    "                    ws.append(w)\n",
    "    return max(hs), max(ws)\n",
    "\n",
    "# check the max height and width in solutions data\n",
    "def get_max_shape_solution(data_dict):\n",
    "    hs, ws = [], []\n",
    "    for section in data_dict.values():\n",
    "        h = len(section)\n",
    "        w = len(section[0])\n",
    "        hs.append(h)\n",
    "        ws.append(w)\n",
    "    return max(hs), max(ws)\n",
    "\n",
    "max_hight, max_weight = 0, 0\n",
    "\n",
    "for data_dict in [train_challenges, eval_challenges, test_data]:\n",
    "    max_h, max_w = get_max_shape_challenge(data_dict)\n",
    "    max_hight = max(max_hight, max_h)\n",
    "    max_weight = max(max_weight, max_w)\n",
    "\n",
    "print(\"Max height and width:\",max_hight, max_weight)\n",
    "\n",
    "\n",
    "for data_dict in [train_solutions, eval_solutions]:\n",
    "    max_h, max_w = get_max_shape_solution(data_dict)\n",
    "    max_hight = max(max_hight, max_h)\n",
    "    max_weight = max(max_weight, max_w)\n",
    "\n",
    "print(\"Max height and width:\",max_hight, max_weight)\n",
    "\n",
    "# reshpae to max_height and max_width\n",
    "def pad_matrix(x, max_height, max_width):\n",
    "    return F.pad(x, (0, max_width - x.shape[1], 0, max_height - x.shape[0]), mode='constant', value=0)\n",
    "\n",
    "def pad_all_challenges(data_dict, max_height, max_width):\n",
    "    result = {}\n",
    "    for key, section in data_dict.items():\n",
    "        result[key] = {}\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase not in section:\n",
    "                continue\n",
    "            result[key][phase] = []\n",
    "            for pair in section[phase]:\n",
    "                new_pair = {}\n",
    "                for k, arr in pair.items():\n",
    "                    arr_t = torch.tensor(arr, dtype=torch.float)\n",
    "                    new_pair[k] = pad_matrix(arr_t, max_height, max_width)\n",
    "                result[key][phase].append(new_pair)\n",
    "    return result\n",
    "\n",
    "def pad_all_solutions(data_dict, max_height, max_width):\n",
    "    result = {}\n",
    "    for key, section in data_dict.items():\n",
    "        result[key] = []\n",
    "        for arr in section:\n",
    "            arr_t = torch.tensor(arr, dtype=torch.float)\n",
    "            result[key].append(pad_matrix(arr_t, max_height, max_width))\n",
    "    return result\n",
    "\n",
    "# insert data in list\n",
    "result_datasets = []\n",
    "for data in [train_challenges, eval_challenges, test_data]:\n",
    "    padded_data_challenges = pad_all_challenges(data, max_hight, max_weight)\n",
    "    result_datasets.append(padded_data_challenges)\n",
    "\n",
    "for data in [train_solutions, eval_solutions]:\n",
    "    padded_data_solutions = pad_all_solutions(data, max_hight, max_weight)\n",
    "    result_datasets.append(padded_data_solutions)\n",
    "\n",
    "train_challenges, eval_challenges, test_data, train_solutions, eval_solutions = result_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a474a670",
   "metadata": {},
   "source": [
    "- LightGBM\n",
    "- Transformers\n",
    "- Graph Neural Networks\n",
    "- Program Synthesis / Sysbolic AI\n",
    "- Neuro-Symbolic AI\n",
    "\n",
    "1. LightGBM\n",
    "2. First of all, CNN + Transformers is the base line.\n",
    "3. Second of all, Vision Transformer + Attension.\n",
    "4. Third of all, Graph Neural Network.\n",
    "5. Last of all, Neuro-Symbolic like hybrid approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8d033a",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d489b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        bathch_size, seq_len, d_model = x.size()\n",
    "        return x.view(bathch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the heads back to the original shape\n",
    "        batch_size, _, seq_len, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d63286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35ba84f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding2D(nn.Module):\n",
    "    def __init__(self, d_model, height, width):\n",
    "        super(PositionalEncoding2D, self).__init__()\n",
    "\n",
    "        if d_model % 4 != 0:\n",
    "            raise ValueError(\"d_model must be divisible by 4 for 2D encoding\")\n",
    "        pe = torch.zeros(d_model, height, width)\n",
    "        d_model = int(d_model / 2)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pos_w = torch.arange(0., width).unsqueeze(1)\n",
    "        pos_h = torch.arange(0., height).unsqueeze(1)\n",
    "\n",
    "        pe[0:d_model:2, :, :] = torch.sin(pos_w * div_term).transpose(0, 1)\n",
    "        pe[1:d_model:2, :, :] = torch.cos(pos_w * div_term).transpose(0, 1)\n",
    "        pe[d_model::2, :, :] = torch.sin(pos_h * div_term).transpose(0, 1)\n",
    "        pe[d_model + 1::2, :, :] = torch.cos(pos_h * div_term).transpose(0, 1)\n",
    "\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe.to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99541ae5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (64) must match the existing size (16) at non-singleton dimension 1.  Target sizes: [16, 64, 64].  Tensor sizes: [16, 64]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pe \u001b[38;5;241m=\u001b[39m \u001b[43mPositionalEncoding2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m pe\n",
      "Cell \u001b[0;32mIn[21], line 13\u001b[0m, in \u001b[0;36mPositionalEncoding2D.__init__\u001b[0;34m(self, d_model, height, width)\u001b[0m\n\u001b[1;32m     10\u001b[0m pos_w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0.\u001b[39m, width)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m pos_h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0.\u001b[39m, height)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mpe\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msin(pos_w \u001b[38;5;241m*\u001b[39m div_term)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m pe[\u001b[38;5;241m1\u001b[39m:d_model:\u001b[38;5;241m2\u001b[39m, :, :] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcos(pos_w \u001b[38;5;241m*\u001b[39m div_term)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m pe[d_model::\u001b[38;5;241m2\u001b[39m, :, :] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msin(pos_h \u001b[38;5;241m*\u001b[39m div_term)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (64) must match the existing size (16) at non-singleton dimension 1.  Target sizes: [16, 64, 64].  Tensor sizes: [16, 64]"
     ]
    }
   ],
   "source": [
    "pe = PositionalEncoding2D(64, 64, 64)\n",
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14f8904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cee217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6340f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding2D(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1309d6f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PositionalEncoding2D.__init__() missing 1 required positional argument: 'width'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m max_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      8\u001b[0m dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m---> 10\u001b[0m transformer \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_vocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_vocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_ff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(src_vocab_size, d_model)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(tgt_vocab_size, d_model)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding \u001b[38;5;241m=\u001b[39m \u001b[43mPositionalEncoding2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers)])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers)])\n",
      "\u001b[0;31mTypeError\u001b[0m: PositionalEncoding2D.__init__() missing 1 required positional argument: 'width'"
     ]
    }
   ],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b7355a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# input to decoder: all tokens except last\u001b[39;00m\n\u001b[1;32m     85\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m tgt_batch[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 86\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, L, vocab)\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# collapse and compute loss against tgt_batch[:,1:]\u001b[39;00m\n\u001b[1;32m     88\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), tgt_batch[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[12], line 23\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, tgt):\n\u001b[0;32m---> 23\u001b[0m     src_mask, tgt_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     src_embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_embedding(src)))\n\u001b[1;32m     25\u001b[0m     tgt_embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_embedding(tgt)))\n",
      "Cell \u001b[0;32mIn[12], line 19\u001b[0m, in \u001b[0;36mTransformer.generate_mask\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m     17\u001b[0m seq_length \u001b[38;5;241m=\u001b[39m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m nopeak_mask \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mtriu(torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m, seq_length, seq_length), diagonal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mbool()\n\u001b[0;32m---> 19\u001b[0m tgt_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtgt_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnopeak_mask\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m src_mask, tgt_mask\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "# --- special tokens ---\n",
    "PAD = 0\n",
    "SOS = 1\n",
    "EOS = 2\n",
    "_reserved_start = 3\n",
    "\n",
    "def flatten_grid(grid):\n",
    "    # grid は list[list[int]] を想定（行優先 flatten）\n",
    "    return [int(v) for row in grid for v in row]\n",
    "\n",
    "# --- build vocab from train_challenges ---\n",
    "unique_vals = set()\n",
    "max_src_len = 0\n",
    "max_tgt_len = 0\n",
    "\n",
    "for cid, chal in train_challenges.items():\n",
    "    for ex in chal.get(\"train\", []):\n",
    "        src_seq = flatten_grid(ex[\"input\"])\n",
    "        tgt_seq = flatten_grid(ex[\"output\"])\n",
    "        unique_vals.update(src_seq)\n",
    "        unique_vals.update(tgt_seq)\n",
    "        max_src_len = max(max_src_len, len(src_seq))\n",
    "        # tgt will include SOS/EOS during encoding\n",
    "        max_tgt_len = max(max_tgt_len, len(tgt_seq) + 2)\n",
    "\n",
    "# deterministic ordering\n",
    "unique_vals = sorted(unique_vals)\n",
    "value2idx = {v: i + _reserved_start for i, v in enumerate(unique_vals)}\n",
    "vocab_size = _reserved_start + len(unique_vals)\n",
    "\n",
    "# --- encode helpers ---\n",
    "def encode_src(grid, pad_len):\n",
    "    seq = flatten_grid(grid)\n",
    "    ids = [value2idx[v] for v in seq]\n",
    "    ids = ids + [PAD] * (pad_len - len(ids))\n",
    "    return ids\n",
    "\n",
    "def encode_tgt(grid, pad_len):\n",
    "    seq = flatten_grid(grid)\n",
    "    ids = [SOS] + [value2idx[v] for v in seq] + [EOS]\n",
    "    ids = ids + [PAD] * (pad_len - len(ids))\n",
    "    return ids\n",
    "\n",
    "# --- prepare tensors ---\n",
    "src_list = []\n",
    "tgt_list = []\n",
    "for cid, chal in train_challenges.items():\n",
    "    for ex in chal.get(\"train\", []):\n",
    "        src_list.append(encode_src(ex[\"input\"], max_src_len))\n",
    "        tgt_list.append(encode_tgt(ex[\"output\"], max_tgt_len))\n",
    "\n",
    "src_tensor = torch.tensor(src_list, dtype=torch.long)\n",
    "tgt_tensor = torch.tensor(tgt_list, dtype=torch.long)\n",
    "\n",
    "class ARCDataset(Dataset):\n",
    "    def __init__(self, src, tgt):\n",
    "        self.src = src\n",
    "        self.tgt = tgt\n",
    "    def __len__(self):\n",
    "        return self.src.size(0)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src[idx], self.tgt[idx]\n",
    "\n",
    "batch_size = 32\n",
    "dataset = ARCDataset(src_tensor, tgt_tensor)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# --- recreate Transformer with correct vocab and seq length ---\n",
    "max_seq_length = max(max_src_len, max_tgt_len)\n",
    "# reuse hyperparams d_model, num_heads, num_layers, d_ff, dropout if defined\n",
    "transformer = Transformer(vocab_size, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout).to(device)\n",
    "\n",
    "# --- training snippet ---\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=1e-4)\n",
    "\n",
    "transformer.train()\n",
    "for epoch in range(3):\n",
    "    epoch_loss = 0.0\n",
    "    for src_batch, tgt_batch in loader:\n",
    "        src_batch = src_batch.to(device)\n",
    "        tgt_batch = tgt_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # input to decoder: all tokens except last\n",
    "        decoder_input = tgt_batch[:, :-1]\n",
    "        output = transformer(src_batch, decoder_input)  # (B, L, vocab)\n",
    "        # collapse and compute loss against tgt_batch[:,1:]\n",
    "        loss = criterion(output.contiguous().view(-1, vocab_size), tgt_batch[:, 1:].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"epoch {epoch+1} avg_loss {epoch_loss/len(loader):.4f}\")\n",
    "\n",
    "# --- quick eval example (no teacher forcing loop here, uses same batching) ---\n",
    "transformer.eval()\n",
    "with torch.no_grad():\n",
    "    src_batch, tgt_batch = next(iter(loader))\n",
    "    src_batch = src_batch.to(device)\n",
    "    tgt_batch = tgt_batch.to(device)\n",
    "    out = transformer(src_batch, tgt_batch[:, :-1])\n",
    "    val_loss = criterion(out.contiguous().view(-1, vocab_size), tgt_batch[:, 1:].contiguous().view(-1))\n",
    "    print(\"val loss\", val_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaceac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98cffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.eval()\n",
    "\n",
    "# Generate random sample validation data\n",
    "val_src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "val_tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    val_output = transformer(val_src_data, val_tgt_data[:, :-1])\n",
    "    val_loss = criterion(val_output.contiguous().view(-1, tgt_vocab_size), val_tgt_data[:, 1:].contiguous().view(-1))\n",
    "    print(f\"Validation Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868ec84f",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "- [1] https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch\n",
    "- [2] https://medium.com/data-science/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853\n",
    "- [3] https://www.ibm.com/think/topics/positional-encoding\n",
    "- [4] https://qiita.com/Uking/items/d7bb7da33d2bbe3eeb71"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
